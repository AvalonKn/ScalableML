{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COM6012 Scalable Machine Learning 2019 - Haiping Lu\n",
    "# Lab 3: Matrix factorisation for collaborative filtering recommender systems\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Task 1: To finish in the lab session. **Essential**\n",
    "* Task 2: To finish in the lab session. **Essential**\n",
    "* Task 3: To explore by yourself. **Optional but recommended**\n",
    "\n",
    "**Suggested reading**: \n",
    "* [Collaborative Filtering in Spark](https://spark.apache.org/docs/2.3.2/ml-collaborative-filtering.html)\n",
    "* [DataBricks movie recommendations tutorial](https://github.com/databricks/spark-training/blob/master/website/movie-recommendation-with-mllib.md![image.png](attachment:image.png)). [**DataBricks**](https://en.wikipedia.org/wiki/Databricks) is a company founded by the creators of Apache Spark, checking out their latest packages at [their GitHub page](https://github.com/databricks), e.g., [integration with Scikit-learn](https://github.com/databricks/spark-sklearn), [Deep Learning Pipelines for Apache Spark including TensorFlow](https://github.com/databricks/spark-deep-learning)\n",
    "* [Collaborative Filtering on Wiki](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering)\n",
    "* [Python API on ALS for recommender system](https://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS)\n",
    "* Chapter 15 (particularly Section 15.3.2) of [PySpark tutorial](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf) \n",
    "\n",
    "[**Learn PySpark APIs via Pictures**](https://github.com/jkthompson/pyspark-pictures) (**from recommended/discover repositories** in GitHub, i.e., found via **recommender systems**!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/haipinglu/ScalableML/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish Lab 1 and Lab 2 sessions\n",
    "\n",
    "### You will need to make use of lab 1 and lab 2 notebooks for the Quiz in Lab 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Movie recommendation via collaborative filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic setup unless using shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Collaborative Filtering RecSys\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative filtering\n",
    "[Collaborative filtering](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering) is a classic approach for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix primarily based on the matrix *itself*.  `spark.ml` currently supports **model-based** collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries, using the **alternating least squares (ALS)** algorithm. \n",
    "\n",
    "API: `class pyspark.ml.recommendation.ALS(rank=10, maxIter=10, regParam=0.1, numUserBlocks=10, numItemBlocks=10, implicitPrefs=False, alpha=1.0, userCol='user', itemCol='item', seed=None, ratingCol='rating', nonnegative=False, checkpointInterval=10, intermediateStorageLevel='MEMORY_AND_DISK', finalStorageLevel='MEMORY_AND_DISK', coldStartStrategy='nan')`\n",
    "\n",
    "The following parameters are available:\n",
    "- *rank*: the number of latent factors in the model (defaults to 10).\n",
    "- *maxIter* is the maximum number of iterations to run (defaults to 10).\n",
    "- *regParam*: the regularization parameter in ALS (defaults to 1.0).\n",
    "- *numUserBlocks*/*numItemBlocks*: the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n",
    "- *implicitPrefs*: whether to use the explicit feedback ALS variant or one adapted for implicit feedback data (defaults to false which means using explicit feedback).\n",
    "- *alpha*: a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations (defaults to 1.0).\n",
    "- *nonnegative*: whether or not to use nonnegative constraints for least squares (defaults to false).\n",
    "- *coldStartStrategy*: can be set to “drop” in order to drop any rows in the DataFrame of predictions that contain NaN values (defaults to \"nan\", assigning NaN to a user and/or item factor is not present in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie recommendation\n",
    "\n",
    "In the cells below, we present a small example of collaborative filtering with the data taken from the [MovieLens](http://grouplens.org/datasets/movielens/) project. In this notebook, we use the old 100k dataset (already downloaded in the `Data` folder but you are encouraged to view the source.\n",
    "\n",
    "The dataset looks like this:\n",
    "\n",
    "    196     242     3       881250949\n",
    "    186     302     3       891717742\n",
    "    22      377     1       878887116\n",
    "    244     51      2       880606923\n",
    "    ...\n",
    "\n",
    "This is a **tab separated** list of \n",
    "    \n",
    "    user id | item id | rating | timestamp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Explicit vs. implicit feedback\n",
    "\n",
    "The data above is typically viewed as a user-item matrix with the ratings as the entries and users and items determine the row and column indices. The ratings are **explicit feedback**. The *Mean Squared Error* of rating prediction can be used to evaluate the recommendation model.\n",
    "\n",
    "The ratings can also be used differently. We can treat them as  treated as numbers representing the strength in observations of user actions, i.e., as **implicit feedback** similar to the number of clicks, or the cumulative duration someone spent viewing a movie. Such numbers are then related to the level of confidence in observed user preferences, rather than explicit ratings given to items. The model then tries to find latent factors that can be used to predict the expected preference of a user for an item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cold-start problem\n",
    "\n",
    "The cold-start problem refers to the cases when some users and/or items in the test dataset were not present during training the model. In Spark, these users and items are either assigned `NaN` (not a number, default) or dropped (option `drop`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in and split words (tab separated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"Data/MovieLens100k.data\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the text (`String`) into numbers (`int` or `float`) and then convert RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The warning is benign](https://stackoverflow.com/questions/40845304/runtimewarning-numpy-dtype-size-changed-may-indicate-binary-incompatibility)\n",
    "\n",
    "Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+------+\n",
      "|movieId|rating|timestamp|userId|\n",
      "+-------+------+---------+------+\n",
      "|    242|   3.0|881250949|   196|\n",
      "|    302|   3.0|891717742|   186|\n",
      "|    377|   1.0|878887116|    22|\n",
      "|     51|   2.0|880606923|   244|\n",
      "|    346|   1.0|886397596|   166|\n",
      "+-------+------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: long (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- userId: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the training/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the recommendation model using ALS on the training data. Note we set cold start strategy to `drop` to ensure we don't get NaN evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=10, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model by computing the RMSE on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.9212305273654956\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate top 10 movie recommendations for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userRecs = model.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                                       |\n",
      "+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|471   |[[793, 5.059724], [1450, 4.750056], [344, 4.5776315], [422, 4.4951153], [1589, 4.4870133], [1302, 4.4721546], [465, 4.409152], [686, 4.382967], [667, 4.3724327], [1006, 4.3493056]]  |\n",
      "|463   |[[1166, 4.578314], [1240, 4.4104204], [958, 4.3217916], [854, 4.3081856], [113, 4.27717], [853, 4.209724], [253, 4.166598], [313, 4.161038], [1266, 4.129875], [1142, 4.1059623]]     |\n",
      "|833   |[[1368, 5.1118503], [1643, 4.6412225], [320, 4.5860066], [1367, 4.442153], [1022, 4.4255276], [1597, 4.3913774], [1585, 4.38596], [179, 4.374618], [1070, 4.309567], [1558, 4.305302]]|\n",
      "|496   |[[793, 4.6466503], [1467, 4.335858], [1166, 4.1957674], [251, 4.1790843], [1642, 4.0750113], [1462, 4.060662], [1062, 4.050997], [1589, 4.037816], [1448, 4.0284147], [475, 4.013277]]|\n",
      "|148   |[[1463, 5.411387], [1367, 5.184691], [57, 5.139676], [50, 5.0034723], [408, 4.9057646], [1122, 4.8797708], [902, 4.872445], [511, 4.8387694], [169, 4.781334], [1449, 4.774668]]      |\n",
      "+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs.show(5,  False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate top 10 user recommendations for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieRecs = model.recommendForAllItems(10)\n",
    "movieRecs.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate top 10 movie recommendations for a specified set of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "users.show()\n",
    "userSubsetRecs.show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate top 10 user recommendations for a specified set of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)\n",
    "movies.show()\n",
    "movieSubSetRecs.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfItemFactors=model.itemFactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfItemFactors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`.describe().show()` is very handy to inspect your (big) data for understanding/debugging. Try to use it more often to see.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfItemFactors.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmovies = ratings.select(als.getItemCol()).distinct()\n",
    "allmovies.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1682-1665"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why is there a difference of 17?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercise - Further analysis of the MovieLens data (completing two or more questions is considered as completion of this exercise).\n",
    "* Consider more parameter settings to observe the effecttsm e.g., different values of *rank* and/or *regParam*, `nan` vs `drop` for `coldStartStrategy`, etc.\n",
    "* Use cross validation to select the best model among various parameter settings (reference: Section 15.3.2 of [PySpark tutorial](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf) )\n",
    "* Create a standalone program that carries out collaborative filtering. Run this on a bigger [MovieLens dataset](http://grouplens.org/datasets/movielens/), e.g., 1M, 10M or 20M.\n",
    "\n",
    "* Use 10-fold cross validation (with an 80% training and 20% testing split) to find an average mean average (or squared) error on your test data. Keep your program as parallel as possible. You can create your splits randomly (or any other way you choose!), and don't forget who has access to various variables and who doesn't..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More Recommender Systems via ALS (Optional but recommended)\n",
    "\n",
    "### Databricks tutorial\n",
    "* Complete the tasks in [quiz provided by DataBricks](https://github.com/databricks/spark-training/blob/master/machine-learning/python/MovieLensALS.py) on their data or the data from MovieLens directly. [Solution](https://github.com/databricks/spark-training/blob/master/machine-learning/python/solution/MovieLensALS.py) is posted but you are suggested to try before consulting the solution.\n",
    "\n",
    "### Santander Kaggle competition on produce recommendation\n",
    "* A recent Kaggle competition on [Santander Product Recommendation](https://www.kaggle.com/c/santander-product-recommendation) with a prize of **USD 60,000**, and **1,787 teams** participating. \n",
    "* Follow this [PySpark notebook on an ALS-based solution](https://www.elenacuoco.com/2016/12/22/alternating-least-squares-als-spark-ml/)\n",
    "* Learn the way to consider **implcit preferences** and do the same for other recommendation problems.\n",
    "\n",
    "\n",
    "### Stock Portfolio Recommendations\n",
    "* Follow Chapter 15 of [PySpark tutorial](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf)  to perform [Stock Portfolio Recommendations](https://en.wikipedia.org/wiki/Portfolio_investment))\n",
    "* The data can be downloaded from [Online Retail Data Set](https://archive.ics.uci.edu/ml/datasets/online+retail) at UCI. \n",
    "* Please pay attention to the **data cleaning** step that removes rows containing null value. You may need to do the same when you are dealing with real data.\n",
    "* The data manipulation steps are useful to learn.\n",
    "\n",
    "### Context-aware recommendation\n",
    "\n",
    "* See the method in [Joint interaction with context operation for collaborative filtering](https://www.sciencedirect.com/science/article/pii/S0031320318304242?dgcid=rss_sd_all) and implement it in PySpark\n",
    "* Perform the **time split recommendation** as disscussed in the paper for the above recommender systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
